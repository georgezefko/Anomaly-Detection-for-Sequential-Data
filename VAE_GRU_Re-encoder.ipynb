{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AML_VAE_NO_REnc_Green_mobility.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgezefko/Anomaly-Detection-for-Sequential-Data/blob/main/VAE_GRU_Re-encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSmcjEonkIH3",
        "outputId": "4e85bd5d-198f-45a8-c794-031e1dacf908"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOfF3qEoq1E3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23847968-31f6-40f1-ed48-0dd1a60f3914"
      },
      "source": [
        "!pip3 install pickle5"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUyh7zCf9D8Y"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch import distributions\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import numpy as np\n",
        "from random import randint\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "from torch.nn.parameter import Parameter\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, confusion_matrix, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import pickle5 as pickle\n",
        "import statsmodels.api\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#93D30C\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 14, 10\n",
        "register_matplotlib_converters()\n",
        "\n",
        "batch_size = 64\n",
        "green_mobility = True\n",
        "max=False\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cuda = torch.cuda.is_available()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5vGON-wIjUu"
      },
      "source": [
        "def heartbeat(data_train,data_test_val):\n",
        "  data = np.concatenate((data_train, data_test_val), axis=0)\n",
        "  df = pd.DataFrame(data)\n",
        "  new_columns = list(df.columns)\n",
        "  new_columns[0] = 'target'\n",
        "  df.columns = new_columns\n",
        "  df.target = [int(i) for i in df.target]\n",
        "\n",
        "  normal_df = df[df.target == 1].drop(labels='target', axis=1)\n",
        "  anomaly_df = df[df.target != 1].drop(labels='target', axis=1)\n",
        "  return normal_df,anomaly_df"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rEijV3NjVhI"
      },
      "source": [
        "from scipy.signal import butter,filtfilt\n",
        "# Filter requirements.\n",
        "T = 5.0 # Sample Period\n",
        "fs = 20.0 # sample rate, Hz\n",
        "cutoff = 1.2 # desired cutoff frequency of the filter, Hz , slightly higher than actual 1.2 Hz\n",
        "nyq = 0.5 * fs # Nyquist Frequency\n",
        "order = 2 # sin wave can be approx represented as quadratic\n",
        "n = int(T * fs) # total number of samples\n",
        " \n",
        "def butter_lowpass_filter(data, cutoff, fs, order):\n",
        "  normal_cutoff = cutoff / nyq\n",
        "  # Get the filter coefficients \n",
        "  b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "  y = filtfilt(b, a, data)\n",
        "  return y\n",
        "def smoothing(Normal,Anomaly): \n",
        "  Normal_smooth = pd.DataFrame()\n",
        "  for i in range(Normal.shape[0]):\n",
        "    Normal_smooth = Normal_smooth.append(pd.DataFrame(butter_lowpass_filter(Normal.loc[i].values, cutoff, fs, order)).transpose())\n",
        "  Normal_smooth = Normal_smooth.reset_index(drop = True) \n",
        " \n",
        "  Anomaly_smooth = pd.DataFrame()\n",
        "  for i in range(Anomaly.shape[0]):\n",
        "    Anomaly_smooth = Anomaly_smooth.append(pd.DataFrame(butter_lowpass_filter(Anomaly.loc[i].values, cutoff, fs, order)).transpose())\n",
        "  Anomaly_smooth = Anomaly_smooth.reset_index(drop = True)\n",
        "  return Normal_smooth,Anomaly_smooth"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mpf6fJ3qInwJ"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4AY7fJy9JvZ"
      },
      "source": [
        "if green_mobility:\n",
        "  path = '/content/drive/MyDrive/AdvancedML/data/resampled_df.pickle'\n",
        "  with open(path,'rb') as fh:\n",
        "      data = pickle.load(fh)\n",
        "  data = data[['GM.acc.xyz.z_resampled','IRI_mean']]\n",
        "  data_unstacked = data.iloc[:,0].apply(list).apply(pd.Series)\n",
        "  df = pd.concat([data_unstacked,data['IRI_mean']],axis=1)\n",
        "  normal_df = df[df['IRI_mean'] <= 2].drop(labels='IRI_mean',axis=1).reset_index(drop = True)\n",
        "  anomaly_df = df[df['IRI_mean'] > 2].drop(labels='IRI_mean',axis=1).reset_index(drop = True)\n",
        "else:\n",
        "  data_train = np.loadtxt('/content/drive/MyDrive/AdvancedML/data/ECG5000/ECG5000_TRAIN', delimiter=',')\n",
        "  data_test_val = np.loadtxt('/content/drive/MyDrive/AdvancedML/data/ECG5000/ECG5000_TEST', delimiter=',')[:-1]\n",
        "  normal_df,anomaly_df = heartbeat(data_train,data_test_val)\n",
        "\n",
        "#for smoothing(Needs correction) it has been used only for experimentation\n",
        "#normal_df,anomaly_df=smoothing(normal_df,anomaly_df)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "train_df, val_df = train_test_split(normal_df,test_size=0.33,random_state=RANDOM_SEED)\n",
        "val_df, test_df = train_test_split(val_df,test_size=0.15, random_state=RANDOM_SEED)\n",
        "\n",
        "\n",
        "# ## Scaling your dataset\n",
        "if max:\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  train_df = scaler.fit_transform(train_df)\n",
        "  test_df = scaler.transform(test_df)\n",
        "  val_df = scaler.transform(val_df)\n",
        "  anomaly_df = scaler.fit_transform(anomaly_df)\n",
        "else:\n",
        "  scaler = StandardScaler()\n",
        "  train_df = scaler.fit_transform(train_df)\n",
        "  test_df = scaler.transform(test_df)\n",
        "  val_df = scaler.transform(val_df)\n",
        "  anomaly_df = scaler.fit_transform(anomaly_df)\n",
        "\n",
        "\n",
        "\n",
        "train_df = np.expand_dims(train_df,-1)\n",
        "test_df = np.expand_dims(test_df,-1)\n",
        "val_df = np.expand_dims(val_df,-1)\n",
        "anomaly_df = np.expand_dims(anomaly_df,-1)\n",
        "\n",
        "seq_len = train_df.shape[1]\n",
        "n_features = train_df.shape[2]\n",
        "\n",
        "train_dataset = TensorDataset(torch.from_numpy(train_df).float())\n",
        "test_dataset = TensorDataset(torch.from_numpy(test_df).float())\n",
        "val_dataset = TensorDataset(torch.from_numpy(val_df).float())\n",
        "anomaly_dataset = TensorDataset(torch.from_numpy(anomaly_df).float())\n",
        "\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, pin_memory=cuda,drop_last=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, pin_memory=cuda,drop_last=True)\n",
        "val_loader  = DataLoader(val_dataset, batch_size=batch_size, pin_memory=cuda, drop_last=True)\n",
        "anomaly_loader = DataLoader(anomaly_dataset, batch_size=batch_size, pin_memory=cuda,drop_last=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFAIS1wX9zV2"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, n_features, hidden_size, hidden_layer_depth, latent_length, bidirectional,dropout):\n",
        "\n",
        "    super(Encoder,self).__init__()\n",
        "\n",
        "\n",
        "    self.n_features = n_features\n",
        "    self.hidden_size = hidden_size\n",
        "    self.hidden_layer_depth = hidden_layer_depth\n",
        "    self.latent_length = latent_length\n",
        "    self.bidirectional = bidirectional\n",
        "    self.direction_factor = (2 if self.bidirectional else 1)\n",
        "    \n",
        "\n",
        "\n",
        "    self.rnn1 = nn.GRU( input_size=self.n_features, \n",
        "                       hidden_size=self.hidden_size, \n",
        "                       dropout = dropout, \n",
        "                       num_layers = self.hidden_layer_depth,\n",
        "                       batch_first=True,bidirectional = self.bidirectional)\n",
        "    \n",
        "    \n",
        "      \n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = x.to(device)\n",
        "    batch = x.size(0)\n",
        "    h0 = torch.zeros(hidden_layer_depth*self.direction_factor, batch, self.hidden_size).to(device)\n",
        "\n",
        "    out,hidden_n = self.rnn1(x,h0) \n",
        "    if self.bidirectional:\n",
        "\n",
        "      last_hidden_state = hidden_n.view(hidden_layer_depth, self.direction_factor, batch, self.hidden_size)[-1] #(num_directions, batch, hidden_size)\n",
        "      h_1, h_2 = last_hidden_state[0], last_hidden_state[1]\n",
        "\n",
        "      #X = h_1 + h_2                # Add both states (needs different input size for first linear layer)\n",
        "      hidden = torch.cat((h_1, h_2), 1) # Concatenate both states\n",
        "    else:\n",
        "      hidden = hidden_n[-1, :, :]\n",
        "    return hidden\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vriDHSn6zM8o"
      },
      "source": [
        "class Latent(nn.Module):\n",
        " \n",
        "    def __init__(self, hidden_size, latent_length,bidirectional):\n",
        "        super(Latent, self).__init__()\n",
        "\n",
        "        self.bidirectional = bidirectional\n",
        "        self.direction_factor = (2 if self.bidirectional else 1)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.latent_length = latent_length\n",
        "\n",
        "        self.hidden_to_mean = nn.Linear(self.hidden_size *self.direction_factor, self.latent_length)\n",
        "        self.hidden_to_logvar = nn.Linear(self.hidden_size * self.direction_factor, self.latent_length)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.hidden_to_mean.weight)\n",
        "        nn.init.xavier_uniform_(self.hidden_to_logvar.weight)\n",
        "\n",
        "    def forward(self, cell_output):\n",
        "        \n",
        "        self.latent_mean = self.hidden_to_mean(cell_output)\n",
        "        self.latent_logvar = self.hidden_to_logvar(cell_output)\n",
        "        z_mu = self.latent_mean\n",
        "        z_var = self.latent_logvar\n",
        "        \n",
        "\n",
        "        \n",
        "        std = torch.exp(0.5 * self.latent_logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        sample = eps.mul(std).add_(self.latent_mean)\n",
        "\n",
        "        return sample, z_mu, z_var\n",
        "        "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiM1LVOQ93I_"
      },
      "source": [
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, seq_len, batch_size, hidden_size, hidden_layer_depth, latent_length, n_features,bidirectional):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_len = seq_len\n",
        "    self.hidden_layer_depth = hidden_layer_depth\n",
        "    self.latent_length = latent_length\n",
        "    self.n_features = n_features\n",
        "    self.bidirectional = bidirectional\n",
        "    self.direction_factor = (2 if self.bidirectional else 1)\n",
        "    \n",
        "    \n",
        "\n",
        "    self.rnn1 = nn.GRU(input_size=1,\n",
        "                       hidden_size=self.hidden_size,\n",
        "                       dropout = dropout,\n",
        "                       num_layers=hidden_layer_depth,\n",
        "                       batch_first=True,\n",
        "                       bidirectional= bidirectional\n",
        "                       )\n",
        "\n",
        "    \n",
        "\n",
        "    self.latent_to_hidden = nn.Linear(self.latent_length, self.hidden_size)\n",
        "    self.hidden_to_output = nn.Linear(self.hidden_size*self.direction_factor, self.n_features)\n",
        "\n",
        "    self.decoder_inputs = torch.zeros( self.batch_size, self.seq_len, 1, requires_grad=True).to(device)\n",
        "\n",
        "    nn.init.xavier_uniform_(self.latent_to_hidden.weight)\n",
        "    nn.init.xavier_uniform_(self.hidden_to_output.weight)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = x.to(device)\n",
        "    h_state = self.latent_to_hidden(x)\n",
        "    \n",
        "    h_0 = torch.stack([h_state for _ in range(self.hidden_layer_depth*self.direction_factor)]).to(device)\n",
        "    \n",
        "    decoder_output, _ = self.rnn1(self.decoder_inputs, h_0)\n",
        "    \n",
        "\n",
        "    out = self.hidden_to_output(decoder_output)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hApna9du94k9"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "\n",
        "  def __init__(self, enc,lat,dec):\n",
        "\n",
        "    super(VAE,self).__init__()\n",
        "\n",
        "  \n",
        "    self.enc = enc\n",
        "    self.lat = lat\n",
        "    self.dec = dec\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    encoder_output = self.enc(x)\n",
        "    sample,z_mu,z_var = self.lat(encoder_output)\n",
        "    x_hat = self.dec(sample)\n",
        "    re_encoder_output = self.enc(x_hat)\n",
        "    rec_sample,rec_z,rec_var = self.lat(re_encoder_output)\n",
        "\n",
        "\n",
        "\n",
        "    return x_hat, z_mu, z_var,sample, rec_sample,rec_z,rec_var"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On7-e4mx98FK",
        "outputId": "686e15fd-2471-4a36-bb25-43f475bf0c83"
      },
      "source": [
        "hidden_size = 256\n",
        "hidden_layer_depth = 2\n",
        "latent_length = 10\n",
        "dropout = 0.5\n",
        "bidirectional = True\n",
        "# encoder\n",
        "encoder = Encoder( n_features, hidden_size, hidden_layer_depth, latent_length,bidirectional, dropout)\n",
        "\n",
        "#latent\n",
        "latent = Latent( hidden_size, latent_length, bidirectional)\n",
        "\n",
        "# decoder\n",
        "decoder = Decoder(seq_len, batch_size, hidden_size, hidden_layer_depth, latent_length, n_features,bidirectional)\n",
        "\n",
        "# vae\n",
        "model = VAE(encoder,latent,decoder).to(device)\n",
        "\n",
        "\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VAE(\n",
            "  (enc): Encoder(\n",
            "    (rnn1): GRU(1, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  )\n",
            "  (lat): Latent(\n",
            "    (hidden_to_mean): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (hidden_to_logvar): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            "  (dec): Decoder(\n",
            "    (rnn1): GRU(1, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "    (latent_to_hidden): Linear(in_features=10, out_features=256, bias=True)\n",
            "    (hidden_to_output): Linear(in_features=512, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlC50fY332vU"
      },
      "source": [
        "def loss(X, x_hat, z_mu, z_var,sample, rec_sample, rec_z, rec_var, criterion):\n",
        "  \n",
        "\n",
        "  if criterion == 'MSELoss':\n",
        "    criterion = nn.MSELoss(size_average=False,reduction='sum')\n",
        "  elif criterion == 'SmoothL1Loss':\n",
        "    criterion = nn.SmoothL1Loss(size_average=False,reduction='sum')\n",
        "\n",
        "  recon_loss = criterion(x_hat, X)\n",
        "  latent_recon_loss = criterion(sample,rec_sample)\n",
        "  KL_loss = -0.5 * torch.mean(1 + z_var - z_mu**2 - torch.exp(z_var))\n",
        "  KL_loss_latent = -0.5 * torch.mean(1 + rec_var - rec_z**2 - torch.exp(rec_var))\n",
        "  elbo_loss = recon_loss +  KL_loss\n",
        "  total_loss = recon_loss +  KL_loss + latent_recon_loss + KL_loss_latent\n",
        "\n",
        "  return total_loss,elbo_loss, KL_loss,KL_loss_latent"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc5coPBu-F4m"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.0006796395231964973) # lr derived from optimazation method\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p8YTCYI-Izs",
        "outputId": "45b3a308-2d22-4bd1-d4fa-25b053cd4cb8"
      },
      "source": [
        "#Test model\n",
        "X= next(iter(train_loader))\n",
        "x = X[0].to(device)\n",
        "outputs = model(x.float())\n",
        "\n",
        "\n",
        "x_hat, z_mu, z_var,sample,rec_sample,rec_z,rec_var = outputs\n",
        "\n",
        "print('x',x.shape)\n",
        "print('mu',z_mu.shape)\n",
        "print('var',z_var.shape)\n",
        "print('x_hat',x_hat.shape)\n",
        "print('re_z',rec_z.shape)\n",
        "print('rec_var',rec_var.shape)\n",
        "print('rec_sample',rec_sample.shape)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x torch.Size([64, 369, 1])\n",
            "mu torch.Size([64, 10])\n",
            "var torch.Size([64, 10])\n",
            "x_hat torch.Size([64, 369, 1])\n",
            "re_z torch.Size([64, 10])\n",
            "rec_var torch.Size([64, 10])\n",
            "rec_sample torch.Size([64, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6WULzxfkWRw",
        "outputId": "5e91bf3f-0a25-4d12-bba7-191a420dc160"
      },
      "source": [
        "#Test loss\n",
        "total_loss, elbo_loss, KL_loss,KL_latent_loss = loss(x, x_hat, z_mu, z_var,sample, rec_sample, rec_z, rec_var,criterion='SmoothL1Loss')\n",
        "\n",
        "print(elbo_loss)\n",
        "print(total_loss)\n",
        "print(KL_loss)\n",
        "print(KL_latent_loss)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(9512.1992, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(9932.0928, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0056, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(0.0055, device='cuda:0', grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q-KBOMKChfa"
      },
      "source": [
        "def train():\n",
        " \n",
        "\n",
        "  model.train()\n",
        "\n",
        "  train_loss = []\n",
        "  train_kl = []\n",
        "  train_total = []\n",
        "  train_latent = []\n",
        "  \n",
        "  \n",
        "\n",
        "  for t, X in enumerate(train_loader):\n",
        "\n",
        "    X =X[0]\n",
        "    X = X.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    x_hat, z_mu, z_var,sample, rec_sample,rec_z,rec_var = model(X)\n",
        "\n",
        "    total_loss,elbo_loss, KL_loss,KL_loss_latent = loss(X, x_hat, z_mu, z_var,sample, rec_sample,rec_z,rec_var, criterion = 'MSELoss')\n",
        "\n",
        "    elbo_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 5)\n",
        "\n",
        "    train_loss.append(elbo_loss.item())\n",
        "    train_kl.append(KL_loss.item())\n",
        "    train_total.append(total_loss.item())\n",
        "    train_latent.append(KL_loss_latent.item())\n",
        "    \n",
        "\n",
        "    optimizer.step()\n",
        "  \n",
        "  train_elbo_loss = np.mean(train_loss)\n",
        "  train_kl_loss = np.mean(train_kl)\n",
        "  train_total_loss = np.mean(train_loss)\n",
        "  train_kl_latent_loss = np.mean(train_latent)\n",
        " \n",
        " \n",
        "\n",
        "  return  train_total_loss, train_elbo_loss, train_kl_loss, train_kl_latent_loss"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vIAvRgoCi71"
      },
      "source": [
        "def test():\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    test_loss = []\n",
        "    test_kl = []\n",
        "    test_total = []\n",
        "    test_latent = []\n",
        "    \n",
        "    for t, x in enumerate(val_loader):\n",
        "\n",
        "      x = x[0]\n",
        "      x = x.to(device)\n",
        "      x_hat, z_mu, z_var,sample, rec_sample,rec_z, rec_var = model(x)\n",
        "      total_loss,elbo_loss, KL_loss, KL_loss_latent = loss(x, x_hat, z_mu, z_var,sample, rec_sample,rec_z,rec_var, criterion = 'MSELoss')\n",
        "\n",
        "      \n",
        "\n",
        "      test_loss.append(elbo_loss.item())\n",
        "      test_kl.append(KL_loss.item())\n",
        "      test_total.append(total_loss.item())\n",
        "      test_latent.append(KL_loss_latent.item())\n",
        "      \n",
        "\n",
        "  test_elbo_loss = np.mean(test_loss)\n",
        "  test_kl_loss = np.mean(test_kl)\n",
        "  test_total_loss = np.mean(test_total)\n",
        "  test_kl_latent_loss = np.mean(test_latent)\n",
        "  \n",
        "\n",
        "  return model.eval(), test_total_loss, test_elbo_loss, test_kl_loss, test_kl_latent_loss"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITTlvdIR41ot"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV_p0xxvCmeg"
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = 10000.0\n",
        "history_elbo = dict(train_elbo=[], test_elbo=[])\n",
        "history_kl = dict(train_kl=[], test_kl=[])\n",
        "history_total = dict(train_total=[], test_total=[])\n",
        "history_latent = dict(train_latent=[], test_latent=[])\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "for e in range(epochs):\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "\n",
        "  train_total_loss, train_elbo_loss, train_kl_loss, train_kl_latent_loss = train()\n",
        "  model, test_total_loss, test_elbo_loss, test_kl_loss, test_kl_latent_loss = test()\n",
        "\n",
        "  end_time = time.time()\n",
        "\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "  if best_loss > test_elbo_loss:\n",
        "    best_test_loss = test_elbo_loss\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  print(f'Epoch: {e+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\t train total loss {train_total_loss} |train elbo loss {train_elbo_loss}| train kl loss {train_kl_loss} | train kl_lat loss {train_kl_latent_loss}') \n",
        "  print(f'\\t val total loss {test_total_loss}    | val elbo loss {test_elbo_loss}  | val kl loss {test_kl_loss}    |val kl_lat loss {test_kl_latent_loss}') \n",
        "\n",
        "\n",
        "  history_elbo['train_elbo'].append(train_elbo_loss)\n",
        "  history_elbo['test_elbo'].append(test_elbo_loss)\n",
        "\n",
        "  history_total['train_total'].append(train_total_loss)\n",
        "  history_total['test_total'].append(test_total_loss)\n",
        "\n",
        "  history_kl['train_kl'].append(train_kl_loss)\n",
        "  history_kl['test_kl'].append(test_kl_loss)\n",
        "\n",
        "  history_latent['train_latent'].append(train_kl_latent_loss)\n",
        "  history_latent['test_latent'].append(test_kl_latent_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T77GWqiPglHK"
      },
      "source": [
        "# save the model\n",
        "MODEL_PATH = 'model.pth'\n",
        "torch.save(model, MODEL_PATH)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaXDAlImgsUD"
      },
      "source": [
        "fig = plt.figure(figsize=(20,15))\n",
        "\n",
        "\n",
        "fig.add_subplot(2,2,1)\n",
        "plt.plot(list(range(0,len(history_elbo['train_elbo']))), history_elbo['train_elbo'],'r')\n",
        "plt.plot(list(range(0,len(history_elbo['test_elbo']))), history_elbo['test_elbo'],'g')\n",
        "plt.legend(['Train ELBO','Val ELBO'])\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('ELBO Loss')\n",
        "\n",
        "\n",
        "fig.add_subplot(2,2,2)\n",
        "plt.plot(list(range(0,len(history_total['train_total']))), history_total['train_total'], 'r')\n",
        "plt.plot(list(range(0,len(history_total['test_total']))), history_total['test_total'],'g')\n",
        "plt.legend(['Train Total Loss','Validation Total Loss'])\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Total Loss')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fig.add_subplot(2,2, 3)\n",
        "plt.plot(list(range(0,len(history_kl['train_kl']))), history_kl['train_kl'], 'r')\n",
        "plt.plot(list(range(0,len(history_kl['test_kl']))), history_kl['test_kl'],'g')\n",
        "plt.legend(['Train KL','Val KL'])\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('KL Loss')\n",
        "\n",
        "\n",
        "fig.add_subplot(2,2, 4)\n",
        "plt.plot(list(range(0,len(history_latent['train_latent']))), history_latent['train_latent'], 'r')\n",
        "plt.plot(list(range(0,len(history_latent['test_latent']))), history_latent['test_latent'],'g')\n",
        "plt.legend(['Train R-KL','Val R-KL'])\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('KL Loss Re-encoder')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#plt.savefig('/content/drive/MyDrive/AdvancedML/data/ecg_losses_vraebigru.png')\n",
        "plt.show();\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EagJnGMky8To"
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aoGYPvB0uaZ"
      },
      "source": [
        "def predict(model, dataset):\n",
        "    predictions = []\n",
        "    enc_mean = []\n",
        "    rec_mean = []\n",
        "    with torch.no_grad():\n",
        "        model = model.eval()\n",
        "    for t, X in enumerate(dataset):\n",
        "        X = X[0]\n",
        "        x = X.to(device)\n",
        "        x_hat, z_mu, z_var,sample, rec_sample,rec_z, rec_var = model(x)\n",
        "\n",
        "        x_hat = x_hat.cpu().detach().numpy()\n",
        "        z_mu = z_mu.cpu().detach().numpy()\n",
        "        rec_z = rec_z.cpu().detach().numpy()\n",
        "\n",
        "        x_hat = np.squeeze(x_hat, axis=2)\n",
        "        predictions.append(x_hat)\n",
        "        enc_mean.append(z_mu)\n",
        "        rec_mean.append(rec_z)\n",
        "       \n",
        "    predictions = np.concatenate(predictions, axis=0, out= None)\n",
        "    enc_mean = np.concatenate(enc_mean, axis=0, out= None)\n",
        "    rec_mean = np.concatenate(rec_mean, axis=0, out= None)\n",
        "    return predictions,enc_mean,rec_mean"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9eeCMA5zxc_"
      },
      "source": [
        "predict_train,mu,re_mu = predict(model, train_loader)\n",
        "predict_anomaly, mu_anomaly, re_mu_anomaly= predict(model, anomaly_loader)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBZ1BuhwE2h8"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def projections(normal,anomaly):\n",
        "  mn = pd.DataFrame(normal)\n",
        "  mn['label']='Normal'\n",
        "  ma = pd.DataFrame(anomaly)\n",
        "  ma['label']='Anomaly'\n",
        "  m = mn.append(ma)\n",
        "\n",
        "\n",
        "  data_subset = m.iloc[:,0:10].values\n",
        "  time_start = time.time()\n",
        "  tsne = TSNE(n_components=2, verbose=1, perplexity=20, n_iter=1000)\n",
        "  tsne_results = tsne.fit_transform(data_subset)\n",
        "  print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "\n",
        "  m['Projection-1'] = tsne_results[:,0]\n",
        "  m['Projection-2'] = tsne_results[:,1]\n",
        "  plt.figure(figsize=(16,10))\n",
        "  sns.scatterplot(\n",
        "    x='Projection-1', y='Projection-2',\n",
        "    hue=\"label\",\n",
        "    palette=sns.color_palette(\"hls\", 2),\n",
        "    data=m,\n",
        "    legend=\"full\",\n",
        "    alpha=0.9)\n",
        "  #plt.savefig('/content/drive/MyDrive/AdvancedML/data/ecg_encoder_latent.png')\n",
        "  plt.show();"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO-BSUZ2JwA3"
      },
      "source": [
        "#Projections encoder\n",
        "projections(mu,mu_anomaly)\n",
        "# Projections re-encoder\n",
        "projections(re_mu,re_mu_anomaly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPJOw5Zo08go"
      },
      "source": [
        "def lillefor(data,predict,test=True):\n",
        "  if test:\n",
        "  #Lillefors test on test normal\n",
        "    residual = (data - predict)\n",
        "    ksstat = np.zeros(residual.shape[0])\n",
        "    for i in range(residual.shape[0]):\n",
        "     ksstat[i] = statsmodels.stats.diagnostic.lilliefors(residual[i,], dist='norm', pvalmethod='table')[0]\n",
        "    sns.distplot(ksstat, bins=40, kde=True)\n",
        "    #plt.savefig('/content/drive/MyDrive/AdvancedML/data/bigru_testnormal_rmse.png')\n",
        "    plt.show()\n",
        "    return ksstat\n",
        "  else:\n",
        "    #MSE on test normal\n",
        "    sq_loss = np.mean(( data- predict)**2, axis=1)\n",
        "    sns.distplot(sq_loss, bins=40, kde=True)\n",
        "    plt.show()\n",
        "    return sq_loss"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2Lah1K51hnE"
      },
      "source": [
        "train_df = train_df[:predict_train.shape[0]]\n",
        "train_original = np.squeeze(train_df,axis=2)\n",
        "train_recon = lillefor(train_original,predict_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAbDy_Wx1ynw"
      },
      "source": [
        "predict_test,_,_ = predict(model, test_loader)\n",
        "test_df=test_df[:predict_test.shape[0]]\n",
        "test_original = np.squeeze(test_df,axis=2)\n",
        "test_recon = lillefor(test_original,predict_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhCybFsyyqCg"
      },
      "source": [
        "anomaly_df = anomaly_df[:predict_anomaly.shape[0]]\n",
        "anomaly_original = np.squeeze(anomaly_df)\n",
        "anomaly_recon = lillefor(anomaly_original,predict_anomaly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZQzyw1nhpXr"
      },
      "source": [
        "def plot_prediction(data, reconstructed, title, ax):\n",
        "  ax.plot(data,'r', label='true')\n",
        "  ax.plot(reconstructed, 'b',label='reconstructed')\n",
        "  ax.set_title(f'{title} (Total loss: {np.around(np.mean(( data- reconstructed)**2), 2)})')\n",
        "  ax.legend()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgSvMnRwhuGL"
      },
      "source": [
        "fig, axs = plt.subplots(nrows=2,ncols=6,sharey=True,sharex=True,figsize=(22, 8))\n",
        "\n",
        "for i, data in enumerate(test_original[:6]):\n",
        "  plot_prediction(data, predict_test[i], title='Normal', ax=axs[0, i])\n",
        "\n",
        "for i, data in enumerate(anomaly_original[:6]):\n",
        "  plot_prediction(data, predict_anomaly[i], title='Anomaly', ax=axs[1, i])\n",
        "\n",
        "fig.tight_layout();\n",
        "#plt.savefig('/content/drive/MyDrive/AdvancedML/data/green_recon_smooth_vraegruuni.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAwebGjSVLcI"
      },
      "source": [
        "label = np.concatenate((np.zeros(len(test_recon)),np.ones(len(anomaly_recon))))\n",
        "#total_losses = np.concatenate((train_sq_loss,anomaly_sq_loss),axis=0)\n",
        "total_recon = np.concatenate((test_recon,anomaly_recon),axis=0)\n",
        "error_df = pd.DataFrame({'Reconstruction_error': total_recon,'Label':label})\n",
        "#error_df = error_df.sample(frac=1)\n",
        "error_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zJSoGaoncGJ"
      },
      "source": [
        "false_pos_rate, true_pos_rate, thresholds = roc_curve(error_df.Label, error_df.Reconstruction_error.values)\n",
        "roc_auc = auc(false_pos_rate, true_pos_rate,)\n",
        "plt.plot(false_pos_rate, true_pos_rate, linewidth=2, label='AUC = %0.3f'% roc_auc)\n",
        "plt.plot([1,0],[1,0], linewidth=2)\n",
        "plt.xlim([-0.01, 1])\n",
        "plt.ylim([0, 1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Receiver operating characteristic curve (ROC)')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhzXxMvvWbq-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
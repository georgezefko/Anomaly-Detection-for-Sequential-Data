{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "rocfix_AML_VAE_NO_REnc_Green_mobility.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgezefko/Anomaly-Detection-for-Sequential-Data/blob/main/VAE_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSmcjEonkIH3",
        "outputId": "a6d1fa8d-b130-4af9-846e-2306d14b86a1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOfF3qEoq1E3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b081bb1-9989-4028-88f2-aa2eac0f1019"
      },
      "source": [
        "!pip3 install pickle5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUyh7zCf9D8Y"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch import distributions\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import numpy as np\n",
        "from random import randint\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "from torch.nn.parameter import Parameter\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, confusion_matrix, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import pickle5 as pickle\n",
        "import statsmodels.api\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#93D30C\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 14, 10\n",
        "register_matplotlib_converters()\n",
        "\n",
        "batch_size = 64\n",
        "green_mobility = True\n",
        "max=False\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cuda = torch.cuda.is_available()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNLSFPD0kl-_"
      },
      "source": [
        "def heartbeat(data_train,data_test_val):\n",
        "  data = np.concatenate((data_train, data_test_val), axis=0)\n",
        "  df = pd.DataFrame(data)\n",
        "  new_columns = list(df.columns)\n",
        "  new_columns[0] = 'target'\n",
        "  df.columns = new_columns\n",
        "  df.target = [int(i) for i in df.target]\n",
        "\n",
        "  normal_df = df[df.target == 1].drop(labels='target', axis=1)\n",
        "  anomaly_df = df[df.target != 1].drop(labels='target', axis=1)\n",
        "  return normal_df,anomaly_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k1jhGFipF_u"
      },
      "source": [
        "\n",
        "from scipy.signal import butter,filtfilt\n",
        "# Filter requirements.\n",
        "T = 5.0 # Sample Period\n",
        "fs = 20.0 # sample rate, Hz\n",
        "cutoff = 1.2 # desired cutoff frequency of the filter, Hz , slightly higher than actual 1.2 Hz\n",
        "nyq = 0.5 * fs # Nyquist Frequency\n",
        "order = 2 # sin wave can be approx represented as quadratic\n",
        "n = int(T * fs) # total number of samples\n",
        " \n",
        "def butter_lowpass_filter(data, cutoff, fs, order):\n",
        "  normal_cutoff = cutoff / nyq\n",
        "  # Get the filter coefficients \n",
        "  b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "  y = filtfilt(b, a, data)\n",
        "  return y\n",
        "def smoothing(Normal,Anomaly): \n",
        "  Normal_smooth = pd.DataFrame()\n",
        "  for i in range(Normal.shape[0]):\n",
        "    Normal_smooth = Normal_smooth.append(pd.DataFrame(butter_lowpass_filter(Normal.loc[i].values, cutoff, fs, order)).transpose())\n",
        "  Normal_smooth = Normal_smooth.reset_index(drop = True) \n",
        " \n",
        "  Anomaly_smooth = pd.DataFrame()\n",
        "  for i in range(Anomaly.shape[0]):\n",
        "    Anomaly_smooth = Anomaly_smooth.append(pd.DataFrame(butter_lowpass_filter(Anomaly.loc[i].values, cutoff, fs, order)).transpose())\n",
        "  Anomaly_smooth = Anomaly_smooth.reset_index(drop = True)\n",
        "  return Normal_smooth,Anomaly_smooth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSyyEuEDiPWl"
      },
      "source": [
        "if green_mobility:\n",
        "  path = '/content/drive/MyDrive/AdvancedML/data/resampled_df.pickle'\n",
        "  with open(path,'rb') as fh:\n",
        "      data = pickle.load(fh)\n",
        "  data = data[['GM.acc.xyz.z_resampled','IRI_mean']]\n",
        "  data_unstacked = data.iloc[:,0].apply(list).apply(pd.Series)\n",
        "  df = pd.concat([data_unstacked,data['IRI_mean']],axis=1)\n",
        "  normal_df = df[df['IRI_mean'] <= 2].drop(labels='IRI_mean',axis=1).reset_index(drop = True)\n",
        "  anomaly_df = df[df['IRI_mean'] > 2].drop(labels='IRI_mean',axis=1).reset_index(drop = True)\n",
        "else:\n",
        "  data_train = np.loadtxt('/content/drive/MyDrive/AdvancedML/data/ECG5000/ECG5000_TRAIN', delimiter=',')\n",
        "  data_test_val = np.loadtxt('/content/drive/MyDrive/AdvancedML/data/ECG5000/ECG5000_TEST', delimiter=',')[:-1]\n",
        "  normal_df,anomaly_df = heartbeat(data_train,data_test_val)\n",
        "\n",
        "#for smoothing(Needs correction) it has been used only for experimentation\n",
        "#normal_df,anomaly_df=smoothing(normal_df,anomaly_df)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "train_df, val_df = train_test_split(Normal,test_size=0.33,random_state=RANDOM_SEED)\n",
        "val_df, test_df = train_test_split(val_df,test_size=0.15, random_state=RANDOM_SEED)\n",
        "\n",
        "\n",
        "# ## Scaling your dataset\n",
        " if max:\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  train_df = scaler.fit_transform(train_df)\n",
        "  test_df = scaler.transform(test_df)\n",
        "  val_df = scaler.transform(val_df)\n",
        "  anomaly_df = scaler.fit_transform(Anomaly)\n",
        " else:\n",
        "  scaler = StandardScaler()\n",
        "  train_df = scaler.fit_transform(train_df)\n",
        "  test_df = scaler.transform(test_df)\n",
        "  val_df = scaler.transform(val_df)\n",
        "  anomaly_df = scaler.fit_transform(Anomaly)\n",
        "\n",
        "\n",
        "\n",
        "train_df = np.expand_dims(train_df,-1)\n",
        "test_df = np.expand_dims(test_df,-1)\n",
        "val_df = np.expand_dims(val_df,-1)\n",
        "anomaly_df = np.expand_dims(anomaly_df,-1)\n",
        "\n",
        "seq_len = train_df.shape[1]\n",
        "n_features = train_df.shape[2]\n",
        "\n",
        "train_dataset = TensorDataset(torch.from_numpy(train_df).float())\n",
        "test_dataset = TensorDataset(torch.from_numpy(test_df).float())\n",
        "val_dataset = TensorDataset(torch.from_numpy(val_df).float())\n",
        "anomaly_dataset = TensorDataset(torch.from_numpy(anomaly_df).float())\n",
        "\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, pin_memory=cuda,drop_last=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, pin_memory=cuda,drop_last=True)\n",
        "val_loader  = DataLoader(val_dataset, batch_size=batch_size, pin_memory=cuda, drop_last=True)\n",
        "anomaly_loader = DataLoader(anomaly_dataset, batch_size=batch_size, pin_memory=cuda,drop_last=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFAIS1wX9zV2"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, n_features, hidden_size, hidden_layer_depth, latent_length, bidirectional,dropout):\n",
        "\n",
        "    super(Encoder,self).__init__()\n",
        "\n",
        "\n",
        "    self.n_features = n_features\n",
        "    self.hidden_size = hidden_size\n",
        "    self.hidden_layer_depth = hidden_layer_depth\n",
        "    self.latent_length = latent_length\n",
        "    self.bidirectional = bidirectional\n",
        "    self.direction_factor = (2 if self.bidirectional else 1)\n",
        "    \n",
        "\n",
        "\n",
        "    self.rnn1 = nn.GRU( input_size=self.n_features, \n",
        "                       hidden_size=self.hidden_size, \n",
        "                       dropout = dropout, \n",
        "                       num_layers = self.hidden_layer_depth,\n",
        "                       batch_first=True,bidirectional = self.bidirectional)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = x.to(device)\n",
        "    batch = x.size(0)\n",
        "    h0 = torch.zeros(hidden_layer_depth*self.direction_factor, batch, self.hidden_size).to(device)\n",
        "\n",
        "    out,hidden_n = self.rnn1(x,h0) \n",
        "  \n",
        "    if self.bidirectional:\n",
        "\n",
        "      last_hidden_state = hidden_n.view(hidden_layer_depth, self.direction_factor, batch, self.hidden_size)[-1] #(num_directions, batch, hidden_size)\n",
        "      \n",
        "      h_1, h_2 = last_hidden_state[0], last_hidden_state[1]\n",
        "      \n",
        "      #X = h_1 + h_2                # Add both states (needs different input size for first linear layer)\n",
        "      hidden = torch.cat((h_1, h_2), 1)  # Concatenate both states\n",
        "      \n",
        "    else:\n",
        "      hidden = hidden_n[-1, :, :]\n",
        "     \n",
        "    \n",
        "\n",
        "    return hidden\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vriDHSn6zM8o"
      },
      "source": [
        "class Latent(nn.Module):\n",
        " \n",
        "    def __init__(self, hidden_size, latent_length,bidirectional):\n",
        "        super(Latent, self).__init__()\n",
        "\n",
        "        self.bidirectional = bidirectional\n",
        "        self.direction_factor = (2 if self.bidirectional else 1)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.latent_length = latent_length\n",
        "\n",
        "        self.hidden_to_mean = nn.Linear(self.hidden_size*self.direction_factor, self.latent_length)\n",
        "        self.hidden_to_logvar = nn.Linear(self.hidden_size*self.direction_factor, self.latent_length)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.hidden_to_mean.weight)\n",
        "        nn.init.xavier_uniform_(self.hidden_to_logvar.weight)\n",
        "\n",
        "    def forward(self, cell_output):\n",
        "        \n",
        "        self.latent_mean = self.hidden_to_mean(cell_output)\n",
        "        self.latent_logvar = self.hidden_to_logvar(cell_output)\n",
        "        z_mu = self.latent_mean\n",
        "        z_var = self.latent_logvar\n",
        "        \n",
        "\n",
        "        \n",
        "        std = torch.exp(0.5 * self.latent_logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        sample = eps.mul(std).add_(self.latent_mean)\n",
        "\n",
        "        return sample, z_mu, z_var\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiM1LVOQ93I_"
      },
      "source": [
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, seq_len, batch_size, hidden_size, hidden_layer_depth, latent_length, n_features,bidirectional):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_len = seq_len\n",
        "    self.hidden_layer_depth = hidden_layer_depth\n",
        "    self.latent_length = latent_length\n",
        "    self.n_features = n_features\n",
        "    self.bidirectional = bidirectional\n",
        "    self.direction_factor = (2 if self.bidirectional else 1)\n",
        "    \n",
        "    \n",
        "\n",
        "    self.rnn1 = nn.GRU(input_size=1,\n",
        "                       hidden_size=self.hidden_size,\n",
        "                       dropout = dropout,\n",
        "                       num_layers=hidden_layer_depth,\n",
        "                       batch_first=True,\n",
        "                       bidirectional= bidirectional\n",
        "                       )\n",
        "\n",
        "    \n",
        "\n",
        "    self.latent_to_hidden = nn.Linear(self.latent_length,self.hidden_size)\n",
        "    self.hidden_to_output = nn.Linear(self.hidden_size*self.direction_factor, self.n_features)\n",
        "\n",
        "    self.decoder_inputs = torch.zeros( self.batch_size, self.seq_len, 1, requires_grad=True).to(device)\n",
        "\n",
        "    nn.init.xavier_uniform_(self.latent_to_hidden.weight)\n",
        "    nn.init.xavier_uniform_(self.hidden_to_output.weight)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = x.to(device)\n",
        "    h_state = self.latent_to_hidden(x)\n",
        "    \n",
        "    h_0 = torch.stack([h_state for _ in range(self.hidden_layer_depth*self.direction_factor)]).to(device)\n",
        "    \n",
        "    decoder_output, _ = self.rnn1(self.decoder_inputs, h_0)\n",
        "    \n",
        "    out = self.hidden_to_output(decoder_output)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hApna9du94k9"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "\n",
        "  def __init__(self, enc,lat,dec):\n",
        "\n",
        "    super(VAE,self).__init__()\n",
        "\n",
        "  \n",
        "    self.enc = enc\n",
        "    self.lat = lat\n",
        "    self.dec = dec\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    encoder_output = self.enc(x)\n",
        "    sample,z_mu,z_var = self.lat(encoder_output)\n",
        "    x_hat = self.dec(sample)\n",
        "\n",
        "\n",
        "\n",
        "    return x_hat, z_mu, z_var "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On7-e4mx98FK",
        "outputId": "a658e751-7267-4e8d-a256-01a6796c02ac"
      },
      "source": [
        "hidden_size = 256\n",
        "hidden_layer_depth = 2\n",
        "latent_length = 10\n",
        "dropout = 0.5\n",
        "bidirectional = True\n",
        "# encoder\n",
        "encoder = Encoder( n_features, hidden_size, hidden_layer_depth, latent_length,bidirectional, dropout)\n",
        "\n",
        "#latent\n",
        "latent = Latent( hidden_size, latent_length, bidirectional)\n",
        "\n",
        "# decoder\n",
        "decoder = Decoder(seq_len, batch_size, hidden_size, hidden_layer_depth, latent_length, n_features,bidirectional)\n",
        "\n",
        "# vae\n",
        "model = VAE(encoder,latent,decoder).to(device)\n",
        "\n",
        "\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VAE(\n",
            "  (enc): Encoder(\n",
            "    (rnn1): GRU(1, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  )\n",
            "  (lat): Latent(\n",
            "    (hidden_to_mean): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (hidden_to_logvar): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            "  (dec): Decoder(\n",
            "    (rnn1): GRU(1, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "    (latent_to_hidden): Linear(in_features=10, out_features=256, bias=True)\n",
            "    (hidden_to_output): Linear(in_features=512, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlC50fY332vU"
      },
      "source": [
        "def loss(X, x_hat, z_mu, z_var, criterion):\n",
        "  \n",
        "\n",
        "  if criterion == 'MSELoss':\n",
        "    criterion = nn.MSELoss(size_average=False,reduction='sum')\n",
        "  elif criterion == 'SmoothL1Loss':\n",
        "    criterion = nn.SmoothL1Loss(size_average=False,reduction='sum')\n",
        "\n",
        "  recon_loss = criterion(x_hat, X)\n",
        "  KL_loss = -0.5 * torch.mean(1 + z_var - z_mu**2 - torch.exp(z_var))\n",
        "  KL_loss = KL_loss/X.view(-1, X.size(1)).data.shape[0] * X.size(1)\n",
        "  \n",
        "  elbo_loss = recon_loss + KL_loss\n",
        "  \n",
        "  return elbo_loss, KL_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc5coPBu-F4m"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.0006796395231964973) # lr derived from optimazation method\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p8YTCYI-Izs"
      },
      "source": [
        "#test of the model\n",
        "X= next(iter(train_loader))\n",
        "x = X[0].to(device)\n",
        "outputs = model(x.float())\n",
        "\n",
        "\n",
        "x_hat, z_mu, z_var = outputs\n",
        "\n",
        "print('x',x.shape)\n",
        "print('mu',z_mu.shape)\n",
        "print('var',z_var.shape)\n",
        "print('x_hat',x_hat.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6WULzxfkWRw"
      },
      "source": [
        "#test on losses\n",
        "elbo_loss, KL_loss = loss(x, x_hat, z_mu, z_var,criterion='MSELoss')\n",
        "\n",
        "print(elbo_loss)\n",
        "print(KL_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q-KBOMKChfa"
      },
      "source": [
        "def train():\n",
        " \n",
        "\n",
        "  model.train()\n",
        "\n",
        "  train_loss = []\n",
        "  train_kl = []\n",
        "  \n",
        "  \n",
        "\n",
        "  for t, X in enumerate(train_loader):\n",
        "\n",
        "    X =X[0]\n",
        "    \n",
        "    X = X.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    x_hat, z_mu, z_var= model(X)\n",
        "\n",
        "    elbo_loss, KL_loss = loss(X, x_hat, z_mu, z_var,criterion='MSELoss')\n",
        "   \n",
        "\n",
        "    elbo_loss.backward()\n",
        "\n",
        "    train_loss.append(elbo_loss.item())\n",
        "    train_kl.append(KL_loss.item())\n",
        "    \n",
        "\n",
        "    optimizer.step()\n",
        "  \n",
        "  train_elbo_loss = np.mean(train_loss)\n",
        "  train_kl_loss = np.mean(train_kl)\n",
        " \n",
        "\n",
        "  return  train_elbo_loss,train_kl_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vIAvRgoCi71"
      },
      "source": [
        "def test():\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    test_loss = []\n",
        "    test_kl = []\n",
        "    \n",
        "    for t, x in enumerate(val_loader):\n",
        "\n",
        "      x = x[0]\n",
        "      x = x.to(device)\n",
        "      x_hat, z_mu, z_var = model(x)\n",
        "      elbo_loss, KL_loss = loss(x, x_hat, z_mu, z_var,criterion='MSELoss')\n",
        "      \n",
        "      test_loss.append(elbo_loss.item())\n",
        "      test_kl.append(KL_loss.item())\n",
        "      \n",
        "\n",
        "  test_elbo_loss = np.mean(test_loss)\n",
        "  test_kl_loss = np.mean(test_kl)\n",
        "  \n",
        "\n",
        "  return model.eval(), test_elbo_loss,test_kl_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITTlvdIR41ot"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV_p0xxvCmeg"
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = 10000.0\n",
        "history_elbo = dict(train_elbo=[], test_elbo=[])\n",
        "history_kl = dict(train_kl=[], test_kl=[])\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "for e in range(epochs):\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  train_elbo_loss, train_kl_loss = train()\n",
        "  model, test_elbo_loss, test_kl_loss  = test()\n",
        "\n",
        "  end_time = time.time()\n",
        "\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "  if best_loss > test_elbo_loss:\n",
        "    best_test_loss = test_elbo_loss\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  print(f'Epoch: {e+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\t train elbo loss {train_elbo_loss}   | train kl loss {train_kl_loss}') \n",
        "  print(f'\\t val  val elbo loss {test_elbo_loss} | valid kl loss {test_kl_loss}') \n",
        "\n",
        "\n",
        "  history_elbo['train_elbo'].append(train_elbo_loss)\n",
        "  history_elbo['test_elbo'].append(test_elbo_loss)\n",
        "\n",
        "  history_kl['train_kl'].append(train_kl_loss)\n",
        "  history_kl['test_kl'].append(test_kl_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T77GWqiPglHK"
      },
      "source": [
        "# save the model\n",
        "MODEL_PATH = 'model.pth'\n",
        "torch.save(model, MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaXDAlImgsUD"
      },
      "source": [
        "fig = plt.figure(figsize=(20,15))\n",
        "\n",
        "\n",
        "fig.add_subplot(2,2,1)\n",
        "plt.plot(list(range(0,len(history_elbo['train_elbo']))), history_elbo['train_elbo'],'r')\n",
        "plt.plot(list(range(0,len(history_elbo['test_elbo']))), history_elbo['test_elbo'],'g')\n",
        "plt.legend(['Train ELBO','Val ELBO'])\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('ELBO Loss')\n",
        "\n",
        "fig.add_subplot(2,2, 2)\n",
        "plt.plot(list(range(0,len(history_kl['train_kl']))), history_kl['train_kl'], 'r')\n",
        "plt.plot(list(range(0,len(history_kl['test_kl']))), history_kl['test_kl'],'g')\n",
        "plt.legend(['Train KL','Val KL'])\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('KL Loss')\n",
        "\n",
        "#plt.savefig('/content/drive/MyDrive/AdvancedML/data/gru_losses_bidirectional.png')\n",
        "\n",
        "\n",
        "plt.show();\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aoGYPvB0uaZ"
      },
      "source": [
        "def predict(model, dataset):\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        model = model.eval()\n",
        "    for t, X in enumerate(dataset):\n",
        "        X = X[0]\n",
        "        x = X.to(device)\n",
        "        x_hat, z_mu, z_var = model(x)\n",
        "        x_hat = x_hat.cpu().detach().numpy()\n",
        "        x_hat = np.squeeze(x_hat, axis=2)\n",
        "        predictions.append(x_hat)\n",
        "    predictions = np.concatenate(predictions, axis=0, out= None)\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2Lah1K51hnE"
      },
      "source": [
        "lillefor=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3qXCzhh14X2"
      },
      "source": [
        "predict_train = predict(model, train_loader)\n",
        "train_df = train_df[:predict_train.shape[0]]\n",
        "train_original = np.squeeze(train_df,axis=2)\n",
        "\n",
        "if lillefor:\n",
        "  #Lillefors test on train\n",
        "  train_residual = (train_original - predict_train)\n",
        "  train_ksstat = np.zeros(train_residual.shape[0])\n",
        "  for i in range(train_residual.shape[0]):\n",
        "    train_ksstat[i] = statsmodels.stats.diagnostic.lilliefors(train_residual[i,], dist='norm', pvalmethod='table')[0]\n",
        "  sns.distplot(train_ksstat, bins=40, kde=True)\n",
        "  plt.show()\n",
        "else:\n",
        "  #MSE on train\n",
        "  train_sq_loss = np.mean(( train_original- predict_train)**2, axis=1)\n",
        "  sns.distplot(train_sq_loss, bins=40, kde=True)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hnrGaRm2Keo"
      },
      "source": [
        "predict_test = predict(model, test_loader)\n",
        "test_df=test_df[:predict_test.shape[0]]\n",
        "test_original = np.squeeze(test_df,axis=2)\n",
        "\n",
        "if lillefor:\n",
        "  #Lillefors test on test normal\n",
        "  test_residual = (test_original - predict_test)\n",
        "  test_ksstat = np.zeros(test_residual.shape[0])\n",
        "  for i in range(test_residual.shape[0]):\n",
        "    test_ksstat[i] = statsmodels.stats.diagnostic.lilliefors(test_residual[i,], dist='norm', pvalmethod='table')[0]\n",
        "  sns.distplot(test_ksstat, bins=40, kde=True)\n",
        "  #plt.savefig('/content/drive/MyDrive/AdvancedML/data/bigru_testnormal_rmse.png')\n",
        "  plt.show()\n",
        "else:\n",
        "  #MSE on test normal\n",
        "  test_sq_loss = np.mean(( test_original- predict_test)**2, axis=1)\n",
        "  sns.distplot(test_sq_loss, bins=40, kde=True)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJuDH7YJ2cvD"
      },
      "source": [
        "predict_anomaly = predict(model, anomaly_loader)\n",
        "anomaly_df = anomaly_df[:predict_anomaly.shape[0]]\n",
        "\n",
        "if lillefor:\n",
        "  #Lillefors test on anomalies\n",
        "  anomaly_residual = (np.squeeze(anomaly_df) - predict_anomaly)\n",
        "  anomaly_ksstat = np.zeros(anomaly_residual.shape[0])\n",
        "  for i in range(anomaly_residual.shape[0]):\n",
        "    anomaly_ksstat[i] = statsmodels.stats.diagnostic.lilliefors(anomaly_residual[i,], dist='norm', pvalmethod='table')[0]\n",
        "\n",
        "  sns.distplot(anomaly_ksstat, bins=20, kde=True)\n",
        "  #plt.savefig('/content/drive/MyDrive/AdvancedML/data/bigru_anomaly_rmse.png')\n",
        "  plt.show()\n",
        "else:\n",
        "  #MSE on anomalies\n",
        "  anomaly_sq_loss = np.mean(( np.squeeze(anomaly_df) - predict_anomaly)**2, axis=1)\n",
        "  sns.distplot(anomaly_sq_loss, bins=20, kde=True)\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZQzyw1nhpXr"
      },
      "source": [
        "def plot_prediction(data, reconstructed, title, ax):\n",
        "  ax.plot(data,'r', label='true')\n",
        "  ax.plot(reconstructed, 'b',label='reconstructed')\n",
        "  ax.set_title(f'{title} (Total loss: {np.around(np.mean(( data- reconstructed)**2), 2)})')\n",
        "  ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgSvMnRwhuGL"
      },
      "source": [
        "#plot reconstrution on test normal and anomaly\n",
        "fig, axs = plt.subplots(nrows=2,ncols=6,sharey=True,sharex=True,figsize=(22, 8))\n",
        "\n",
        "for i, data in enumerate(test_original[0:6]):\n",
        "  plot_prediction(data, predict_test[i], title='Normal', ax=axs[0, i])\n",
        "\n",
        "for i, data in enumerate(anomaly_df[0:6]):\n",
        "  plot_prediction(data, predict_anomaly[i], title='Anomaly', ax=axs[1, i])\n",
        "\n",
        "fig.tight_layout();\n",
        "#plt.savefig('/content/drive/MyDrive/AdvancedML/data/green_recon_smooth_vaebigru.png')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2u-qda62oTz"
      },
      "source": [
        "#create classification dataframe with results from Lillefors test\n",
        "label = np.concatenate((np.zeros(len(test_ksstat)),np.ones(len(anomaly_ksstat))))\n",
        "#total_losses = np.concatenate((train_sq_loss,anomaly_sq_loss),axis=0)\n",
        "total_ksstats = np.concatenate((test_ksstat,anomaly_ksstat),axis=0)\n",
        "error_df_norm = pd.DataFrame({'ksstat': total_ksstats,'Label':label})\n",
        "#error_df = error_df.sample(frac=1)\n",
        "error_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbxpwhMu2uQU"
      },
      "source": [
        "#ROC-AUC for Lillefors test\n",
        "false_pos_rate, true_pos_rate, thresholds = roc_curve(error_df_norm.Label, error_df_norm.ksstat)\n",
        "roc_auc = auc(false_pos_rate, true_pos_rate,)\n",
        "plt.plot(false_pos_rate, true_pos_rate, linewidth=2, label='AUC = %0.3f'% roc_auc)\n",
        "plt.plot([1,0],[1,0], linewidth=2)\n",
        "plt.xlim([-0.01, 1])\n",
        "plt.ylim([0, 1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Receiver operating characteristic curve (ROC)')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RplRhj-PTna5"
      },
      "source": [
        "#create classification dataframe with results from MSE\n",
        "label = np.concatenate((np.zeros(len(test_sq_loss)),np.ones(len(anomaly_sq_loss))))\n",
        "#total_losses = np.concatenate((train_sq_loss,anomaly_sq_loss),axis=0)\n",
        "total_losses = np.concatenate((test_sq_loss,anomaly_sq_loss),axis=0)\n",
        "error_df = pd.DataFrame({'Reconstruction_error': total_losses,'Label':label})\n",
        "#error_df = error_df.sample(frac=1)\n",
        "error_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-wZm3-eACxc"
      },
      "source": [
        "#ROC-AUC for MSE\n",
        "false_pos_rate, true_pos_rate, thresholds = roc_curve(error_df.Label, error_df.Reconstruction_error.values)\n",
        "roc_auc = auc(false_pos_rate, true_pos_rate,)\n",
        "plt.plot(false_pos_rate, true_pos_rate, linewidth=2, label='AUC = %0.3f'% roc_auc)\n",
        "plt.plot([1,0],[1,0], linewidth=2)\n",
        "plt.xlim([-0.01, 1])\n",
        "plt.ylim([0, 1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Receiver operating characteristic curve (ROC)')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()\n",
        "#plt.savefig('/content/drive/MyDrive/adml/data/ROC_green.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf17cqrQ8Qo4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMVAE_REENCODER_FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgezefko/Anomaly-Detection-for-Sequential-Data/blob/main/VAE_LSTM_RE-Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo7B6NvW0Zrw",
        "outputId": "cdce4f9e-209e-46b8-f228-76614c3cb6c3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU1ZRMAiIlGE",
        "outputId": "bc920444-87a5-4b94-ac0d-ae75e2913f4a"
      },
      "source": [
        "!pip install pickle5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_mSbuf90Wuv"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch import distributions\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import numpy as np\n",
        "from random import randint\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "from torch.nn.parameter import Parameter\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, confusion_matrix, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import pickle5 as pickle\n",
        "import statsmodels.api\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#93D30C\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 14, 10\n",
        "register_matplotlib_converters()\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cuda = torch.cuda.is_available()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw1c2saX0Wu-"
      },
      "source": [
        "num_hidden_dim = 128\n",
        "num_enc_hidden_dim = 64\n",
        "num_layers = 1\n",
        "num_samples = 20\n",
        "dropout_rate = 0.2\n",
        "bidirectional = True\n",
        "latent_size = 15\n",
        "clip = True\n",
        "max_grad_norm = 5\n",
        "max = True\n",
        "\n",
        "batch_size = 64\n",
        "learning_rate = 0.0006796395231964973\n",
        "n_epochs = 100\n",
        "RANDOM_SEED = 42\n",
        "green_mobility = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCbTAPnmmszw"
      },
      "source": [
        "## Load dataset of your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BloSv02EZfLT"
      },
      "source": [
        "def heartbeat(data_train,data_test_val):\n",
        "  data = np.concatenate((data_train, data_test_val), axis=0)\n",
        "  df = pd.DataFrame(data)\n",
        "  new_columns = list(df.columns)\n",
        "  new_columns[0] = 'target'\n",
        "  df.columns = new_columns\n",
        "  df.target = [int(i) for i in df.target]\n",
        "\n",
        "  normal_df = df[df.target == 1].drop(labels='target', axis=1)\n",
        "  anomaly_df = df[df.target != 1].drop(labels='target', axis=1)\n",
        "  return normal_df,anomaly_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oov15oifZgAi"
      },
      "source": [
        "from scipy.signal import butter,filtfilt\n",
        "# Filter requirements.\n",
        "T = 5.0 # Sample Period\n",
        "fs = 20.0 # sample rate, Hz\n",
        "cutoff = 1.2 # desired cutoff frequency of the filter, Hz , slightly higher than actual 1.2 Hz\n",
        "nyq = 0.5 * fs # Nyquist Frequency\n",
        "order = 2 # sin wave can be approx represented as quadratic\n",
        "n = int(T * fs) # total number of samples\n",
        " \n",
        "def butter_lowpass_filter(data, cutoff, fs, order):\n",
        "  normal_cutoff = cutoff / nyq\n",
        "  # Get the filter coefficients \n",
        "  b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "  y = filtfilt(b, a, data)\n",
        "  return y\n",
        "def smoothing(Normal,Anomaly): \n",
        "  Normal_smooth = pd.DataFrame()\n",
        "  for i in range(Normal.shape[0]):\n",
        "    Normal_smooth = Normal_smooth.append(pd.DataFrame(butter_lowpass_filter(Normal.loc[i].values, cutoff, fs, order)).transpose())\n",
        "  Normal_smooth = Normal_smooth.reset_index(drop = True) \n",
        " \n",
        "  Anomaly_smooth = pd.DataFrame()\n",
        "  for i in range(Anomaly.shape[0]):\n",
        "    Anomaly_smooth = Anomaly_smooth.append(pd.DataFrame(butter_lowpass_filter(Anomaly.loc[i].values, cutoff, fs, order)).transpose())\n",
        "  Anomaly_smooth = Anomaly_smooth.reset_index(drop = True)\n",
        "  return Normal_smooth,Anomaly_smooth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8ve52Bp0lJ6"
      },
      "source": [
        "if green_mobility:\n",
        "  path = '/content/drive/MyDrive/AdvancedML/data/resampled_df.pickle'\n",
        "  with open(path,'rb') as fh:\n",
        "      data = pickle.load(fh)\n",
        "  data = data[['GM.acc.xyz.z_resampled','IRI_mean']]\n",
        "  data_unstacked = data.iloc[:,0].apply(list).apply(pd.Series)\n",
        "  df = pd.concat([data_unstacked,data['IRI_mean']],axis=1)\n",
        "  normal_df = df[df['IRI_mean'] <= 2].drop(labels='IRI_mean',axis=1).reset_index(drop = True)\n",
        "  anomaly_df = df[df['IRI_mean'] > 2].drop(labels='IRI_mean',axis=1).reset_index(drop = True)\n",
        "else:\n",
        "  data_train = np.loadtxt('/content/drive/MyDrive/AdvancedML/data/ECG5000/ECG5000_TRAIN', delimiter=',')\n",
        "  data_test_val = np.loadtxt('/content/drive/MyDrive/AdvancedML/data/ECG5000/ECG5000_TEST', delimiter=',')[:-1]\n",
        "  normal_df,anomaly_df = heartbeat(data_train,data_test_val)\n",
        "\n",
        "#for smoothing(Needs correction) it has been used only for experimentation\n",
        "#normal_df,anomaly_df=smoothing(normal_df,anomaly_df)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "train_df, val_df = train_test_split(Normal,test_size=0.33,random_state=RANDOM_SEED)\n",
        "val_df, test_df = train_test_split(val_df,test_size=0.15, random_state=RANDOM_SEED)\n",
        "\n",
        "\n",
        "# ## Scaling your dataset\n",
        " if max:\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  train_df = scaler.fit_transform(train_df)\n",
        "  test_df = scaler.transform(test_df)\n",
        "  val_df = scaler.transform(val_df)\n",
        "  anomaly_df = scaler.fit_transform(Anomaly)\n",
        " else:\n",
        "  scaler = StandardScaler()\n",
        "  train_df = scaler.fit_transform(train_df)\n",
        "  test_df = scaler.transform(test_df)\n",
        "  val_df = scaler.transform(val_df)\n",
        "  anomaly_df = scaler.fit_transform(Anomaly)\n",
        "\n",
        "\n",
        "\n",
        "train_df = np.expand_dims(train_df,-1)\n",
        "test_df = np.expand_dims(test_df,-1)\n",
        "val_df = np.expand_dims(val_df,-1)\n",
        "anomaly_df = np.expand_dims(anomaly_df,-1)\n",
        "\n",
        "num_seq_length = train_df.shape[1]\n",
        "num_features = train_df.shape[2]\n",
        "\n",
        "train_dataset = TensorDataset(torch.from_numpy(train_df).float())\n",
        "test_dataset = TensorDataset(torch.from_numpy(test_df).float())\n",
        "val_dataset = TensorDataset(torch.from_numpy(val_df).float())\n",
        "anomaly_dataset = TensorDataset(torch.from_numpy(anomaly_df).float())\n",
        "\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, pin_memory=cuda,drop_last=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, pin_memory=cuda,drop_last=True)\n",
        "val_loader  = DataLoader(val_dataset, batch_size=batch_size, pin_memory=cuda, drop_last=True)\n",
        "anomaly_loader = DataLoader(anomaly_dataset, batch_size=batch_size, pin_memory=cuda,drop_last=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrRibn1P0WvC"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_features,num_seq_length,num_hidden_dim,num_layers, \n",
        "                 num_enc_hidden_dim,dropout_rate,latent_size,bidirectional):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.num_features = num_features                                                          \n",
        "        self.num_seq_length = num_seq_length                                                                                                             #BiDirectional\n",
        "        self.bidirectional =  bidirectional                                                                \n",
        "        self.num_hidden_dim = num_hidden_dim\n",
        "        self.num_enc_hidden_dim = num_enc_hidden_dim                                                   \n",
        "        self.num_layers = num_layers                                                           \n",
        "        self.dropout_rate = dropout_rate                                                            \n",
        "        #self.linear_dims = (self.num_enc_hidden_dim * self.num_directions, 1)           \n",
        "\n",
        "        self.num_directions = (2 if bidirectional else 1)\n",
        "\n",
        "        self.lstm_enc1 = nn.LSTM(\n",
        "            input_size=self.num_features,                       \n",
        "            hidden_size=self.num_hidden_dim,                      \n",
        "            dropout=self.dropout_rate,\n",
        "            num_layers=self.num_layers,                       \n",
        "            batch_first=True,\n",
        "            bidirectional=self.bidirectional                  \n",
        "        )\n",
        "        \n",
        "\n",
        "        self.lstm_enc1.apply(self.init_weights)\n",
        "        \n",
        "    def init_weights(self, m):\n",
        "        if type(m) == nn.LSTM:\n",
        "            for name, param in m.named_parameters():\n",
        "                if 'bias' in name:\n",
        "                    nn.init.constant(param, 0.01)\n",
        "                elif 'weight' in name:\n",
        "                    nn.init.xavier_normal(param) \n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.reshape((batch_size, self.num_seq_length, self.num_features))\n",
        "        x, (hidden, _) = self.lstm_enc1(x)\n",
        "        x = hidden.reshape((batch_size, self.num_layers*self.num_directions*self.num_hidden_dim))\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OeUS0bvKFal"
      },
      "source": [
        "class Latent(nn.Module):\n",
        "  \n",
        "    def __init__(self, num_layers,num_samples, num_hidden_dim, latent_size,bidirectional):\n",
        "        super(Latent, self).__init__()\n",
        "\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = (2 if bidirectional else 1)\n",
        "        self.num_hidden_dim = num_hidden_dim\n",
        "        self.latent_size = latent_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "        self.hidden_to_mean = nn.Linear(self.num_hidden_dim*self.num_directions*self.num_layers, self.latent_size)\n",
        "        self.hidden_to_logvar = nn.Linear(self.num_hidden_dim*self.num_directions*self.num_layers, self.latent_size)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.hidden_to_mean.weight)\n",
        "        nn.init.xavier_uniform_(self.hidden_to_logvar.weight)\n",
        "\n",
        "    def forward(self, cell_output):\n",
        "        self.latent_mean = self.hidden_to_mean(cell_output)\n",
        "        self.latent_logvar = self.hidden_to_logvar(cell_output)\n",
        "        mean = self.latent_mean\n",
        "        log_var = self.latent_logvar\n",
        "        std = torch.exp(0.5 * self.latent_logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        sample = eps.mul(std).add_(self.latent_mean)\n",
        "        #sample = self.reparameterize(mean,log_var,epsilon_std = 1.0)\n",
        "        return sample, mean, log_var\n",
        "    def reparameterize(self,z_mean,z_log_var,epsilon_std = 1.0):\n",
        "        batch_size = z_mean.size(0)\n",
        "        epsilon = torch.FloatTensor(*[batch_size,self.num_samples, self.latent_size]).normal_(0,epsilon_std).to(device)\n",
        "        z = z_mean.unsqueeze(1) + epsilon * torch.exp(0.5 * z_log_var).unsqueeze(1) \n",
        "        return torch.mean(z, dim = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK-oQQYY0WvD"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_features,num_seq_length, num_hidden_dim, num_enc_hidden_dim,num_layers,dropout_rate,bidirectional,latent_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.num_features = num_features                                                           #Features\n",
        "        self.num_seq_length = num_seq_length                                                       #Sequenc                                                        #BiDirectional\n",
        "        self.bidirectional =  bidirectional                                                                   #LSTM H,C -> GRU H\n",
        "        self.num_hidden_dim = num_hidden_dim\n",
        "        self.num_enc_hidden_dim = num_enc_hidden_dim   \n",
        "        self.num_layers = num_layers                                                            #Layers\n",
        "        self.dropout_rate = dropout_rate                                                            #Dropout\n",
        "        self.latent_size = latent_size\n",
        "        self.num_directions = (2 if bidirectional else 1)\n",
        "\n",
        "        self.latent_to_hidden = nn.Linear(self.latent_size, self.num_enc_hidden_dim*self.num_directions*self.num_layers)\n",
        "\n",
        "        self.lstm_dec1 = nn.LSTM(\n",
        "            input_size=self.num_enc_hidden_dim*self.num_directions*self.num_layers,\n",
        "            hidden_size=self.num_hidden_dim,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout = self.dropout_rate,\n",
        "            batch_first=True,\n",
        "            bidirectional=self.bidirectional     \n",
        "        )\n",
        "        \n",
        "        self.dense_layer = nn.Linear(self.num_hidden_dim*self.num_directions,self.num_features)\n",
        "\n",
        "        self.lstm_dec1.apply(self.init_weights)\n",
        "        self.dense_layer.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if type(m) == nn.LSTM or type(m) == nn.Linear:\n",
        "            for name, param in m.named_parameters():\n",
        "                if 'bias' in name:\n",
        "                    nn.init.constant(param, 0.01)\n",
        "                elif 'weight' in name:\n",
        "                    nn.init.xavier_normal(param) \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.latent_to_hidden(x)\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.repeat(1, self.num_seq_length, self.num_features)\n",
        "        x = x.reshape((batch_size, self.num_seq_length, self.num_enc_hidden_dim*self.num_directions*self.num_layers))\n",
        "        x, hidden = self.lstm_dec1(x)\n",
        "        x = x.reshape((batch_size, self.num_seq_length, self.num_hidden_dim*self.num_directions))\n",
        "        x = self.dense_layer(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vBFBC370WvE"
      },
      "source": [
        "class LSTMVAE(nn.Module):\n",
        "    def __init__(self, num_features,num_seq_length, num_hidden_dim, num_enc_hidden_dim,num_layers,num_samples,dropout_rate,\n",
        "                 bidirectional,latent_size):\n",
        "        super(LSTMVAE, self).__init__()\n",
        "        \n",
        "        self.num_seq_length = num_seq_length\n",
        "        self.num_features = num_features\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden_dim = num_hidden_dim\n",
        "        self.num_enc_hidden_dim = num_enc_hidden_dim\n",
        "        self.bidirectional = bidirectional\n",
        "        self.latent_size = latent_size\n",
        "        self.num_samples = num_samples\n",
        "        self.dropout_rate = dropout_rate\n",
        "        \n",
        "        \n",
        "        self.encoder = Encoder(num_features = num_features,\n",
        "                               num_seq_length = num_seq_length,\n",
        "                               num_hidden_dim = num_hidden_dim,\n",
        "                               num_layers = num_layers, \n",
        "                               num_enc_hidden_dim = num_enc_hidden_dim,\n",
        "                               dropout_rate = dropout_rate,\n",
        "                               latent_size = latent_size,\n",
        "                               bidirectional = bidirectional)\n",
        "        self.latent = Latent(num_layers = num_layers,\n",
        "                             num_samples = num_samples,\n",
        "                             num_hidden_dim = num_hidden_dim, \n",
        "                             latent_size = latent_size,\n",
        "                             bidirectional = latent_size)\n",
        "       \n",
        "        self.decoder = Decoder(num_features = num_features,\n",
        "                               num_seq_length = num_seq_length,\n",
        "                               num_hidden_dim = num_hidden_dim,\n",
        "                               num_layers = num_layers, \n",
        "                               num_enc_hidden_dim = num_enc_hidden_dim,\n",
        "                               dropout_rate = dropout_rate,\n",
        "                               bidirectional = bidirectional,\n",
        "                               latent_size = latent_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.encoder(x)\n",
        "      z, z_mu, z_var = self.latent(x)\n",
        "      x_decoded = self.decoder(z)\n",
        "      ## re encoding\n",
        "      x_decoded_reencode = self.encoder(x_decoded)\n",
        "      z_bar, z_mu_bar, z_var_bar = self.latent(x_decoded_reencode)\n",
        "\n",
        "      return x_decoded,z, z_mu, z_var,z_bar, z_mu_bar, z_var_bar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxwm5BEG0WvF"
      },
      "source": [
        "net = LSTMVAE(num_features,num_seq_length, num_hidden_dim, num_enc_hidden_dim,num_layers,num_samples,dropout_rate,\n",
        "                 bidirectional,latent_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx19G2vq0WvG",
        "outputId": "53f7adad-38be-4f7b-d7ec-5e66cdece51f"
      },
      "source": [
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMVAE(\n",
            "  (encoder): Encoder(\n",
            "    (lstm_enc1): LSTM(1, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "  )\n",
            "  (latent): Latent(\n",
            "    (hidden_to_mean): Linear(in_features=256, out_features=15, bias=True)\n",
            "    (hidden_to_logvar): Linear(in_features=256, out_features=15, bias=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (latent_to_hidden): Linear(in_features=15, out_features=128, bias=True)\n",
            "    (lstm_dec1): LSTM(128, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "    (dense_layer): Linear(in_features=256, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5Ja5rukJmVn"
      },
      "source": [
        "if cuda:\n",
        "  net = net.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCTj7Rw10WvK"
      },
      "source": [
        "#criterion = nn.SmoothL1Loss(size_average=False)\n",
        "criterion = nn.MSELoss(size_average=False)\n",
        "#def ELBO_loss(y, t, z,mu, log_var):\n",
        "def ELBO_loss(x, x_hat,z,mu, log_var,z_bar,mu_bar,log_var_bar):\n",
        "    recon_loss = criterion(x, x_hat)\n",
        "    ## recon latent\n",
        "    latent_loss = criterion(z, z_bar)\n",
        "    \n",
        "    kl = -0.5 * torch.sum(1 + log_var - mu**2 - torch.exp(log_var))\n",
        "    \n",
        "    # latent kl\n",
        "    latent_kl = -0.5 * torch.sum(1 + log_var_bar - mu_bar**2 - torch.exp(log_var_bar))\n",
        "   \n",
        "    # Combining the two terms in the evidence lower bound objective (ELBO) \n",
        "    ELBO = recon_loss + kl\n",
        "    total = recon_loss + kl + latent_loss + latent_kl\n",
        "    \n",
        "    return total,ELBO,kl\n",
        "\n",
        "\n",
        "# define our optimizer\n",
        "# The Adam optimizer works really well with VAEs.\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "loss_function = ELBO_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMXZ0A2a0WvL"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "x= next(iter(train_loader))\n",
        "x = Variable(x[0])\n",
        "if cuda:\n",
        "    x = x.cuda()\n",
        "\n",
        "x_hat,z,z_mu,z_var,z_bar,z_mu_bar, z_var_bar = net(x)\n",
        "\n",
        "total,elbo,kl = loss_function(x,x_hat,z, z_mu,z_var,z_bar,z_mu_bar, z_var_bar)\n",
        "\n",
        "print(x.shape)\n",
        "print(x_hat.shape)\n",
        "print(kl)\n",
        "print(elbo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdiC--ZM0WvM"
      },
      "source": [
        "def train_model(model,train_dataset,val_dataset,num_epochs):\n",
        "    history_elbo = dict(train_elbo=[], val_elbo=[])\n",
        "    history_kl = dict(train_kl=[], val_kl=[])\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 10000.0\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        # Go through  in the training dataset \n",
        "        train_elbo_losses, train_kl_losses = [], []\n",
        "        for t, X in enumerate(train_dataset):\n",
        "            # Index first element of array to return tensor\n",
        "            X = X[0]\n",
        "            # This is an alternative way of putting\n",
        "            # a tensor on the GPU\n",
        "            x = X.to(device)\n",
        "          \n",
        "            x_hat,z,z_mu,z_var,z_bar,z_mu_bar, z_var_bar = net(x)\n",
        "            \n",
        "            total,elbo,kl = loss_function(x,x_hat,z, z_mu,z_var,z_bar,z_mu_bar, z_var_bar)\n",
        "          \n",
        "            optimizer.zero_grad()\n",
        "            total.backward()\n",
        "            optimizer.step()\n",
        "            if clip:\n",
        "              torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = max_grad_norm)\n",
        "          \n",
        "            train_elbo_losses.append(elbo.item())\n",
        "            train_kl_losses.append(kl.item())\n",
        "\n",
        "      # Evaluate, do not propagate gradients\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "        valid_elbo_losses, valid_kl_losses = [], []   \n",
        "        for t, X in enumerate(val_dataset):\n",
        "            X = X[0]\n",
        "            x = X.to(device)\n",
        "            x_hat,z,z_mu,z_var,z_bar,z_mu_bar, z_var_bar = net(x)\n",
        "            \n",
        "            total,elbo,kl = loss_function(x,x_hat,z, z_mu,z_var,z_bar,z_mu_bar, z_var_bar)\n",
        "            \n",
        "            valid_elbo_losses.append(elbo.item())\n",
        "            valid_kl_losses.append(kl.item())\n",
        "      \n",
        "        train_elbo_loss = np.mean(train_elbo_losses)\n",
        "        train_kl_loss = np.mean(train_kl_losses)\n",
        "        valid_elbo_loss = np.mean(valid_elbo_losses)\n",
        "        valid_kl_loss = np.mean(valid_kl_losses)\n",
        "        if valid_elbo_loss < best_loss:\n",
        "            best_loss = valid_elbo_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        print(f'Epoch {epoch}: train elbo loss {train_elbo_loss} train kl loss {train_kl_loss} val elbo loss {valid_elbo_loss} valid kl loss {valid_kl_loss}') \n",
        "\n",
        "      # collecting losses\n",
        "        history_elbo['train_elbo'].append(train_elbo_loss)\n",
        "        history_kl['train_kl'].append(train_kl_loss)\n",
        "        history_elbo['val_elbo'].append(valid_elbo_loss)\n",
        "        history_kl['val_kl'].append(valid_kl_loss)\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model.eval(),history_elbo,history_kl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s4BkQl_0WvM"
      },
      "source": [
        "model, history_elbo,history_total = train_model(net,train_loader,val_loader,n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-hEIsah0WvN"
      },
      "source": [
        "# save the model\n",
        "MODEL_PATH = 'model.pth'\n",
        "torch.save(model, MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2pdSReg0WvN"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(211)\n",
        "plt.plot(list(range(0,len(history_elbo['train_elbo']))), history_elbo['train_elbo'],'r')\n",
        "plt.plot(list(range(0,len(history_elbo['val_elbo']))), history_elbo['val_elbo'],'g')\n",
        "plt.legend(['Train ELBO','Val ELBO'])\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('ELBO Loss')\n",
        "plt.show();\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(212)\n",
        "plt.plot(list(range(0,len(history_total['train_kl']))), history_total['train_kl'], 'r')\n",
        "plt.plot(list(range(0,len(history_total['val_kl']))), history_total['val_kl'],'g')\n",
        "plt.legend(['Train KL','Val KL'])\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('KL divergence')\n",
        "plt.title('Kullback-Leibler Divergence')\n",
        "\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFnG4Nv50WvO"
      },
      "source": [
        "def predict(model, dataset):\n",
        "    predictions = []\n",
        "    enc_mean = []\n",
        "    rec_mean = []\n",
        "    with torch.no_grad():\n",
        "        model = model.eval()\n",
        "    for t, X in enumerate(dataset):\n",
        "        X = X[0]\n",
        "        x = X.to(device)\n",
        "        #x = Variable(x, requires_grad = False)\n",
        "        #x_hat,_,_,_,_,_,_= model(x)\n",
        "        x_hat,z,z_mu, z_var,z_bar, z_mu_bar, z_var_bar = model(x)\n",
        "        x_hat = x_hat.detach().cpu()\n",
        "        z_mu = z_mu.detach().cpu()\n",
        "        z_mu_bar = z_mu_bar.detach().cpu()\n",
        "\n",
        "\n",
        "        x_hat = np.squeeze(x_hat, axis=2)\n",
        "        predictions.append(x_hat)\n",
        "        enc_mean.append(z_mu)\n",
        "        rec_mean.append(z_mu_bar)\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    enc_mean = np.concatenate(enc_mean, axis=0, out= None)\n",
        "    rec_mean = np.concatenate(rec_mean, axis=0, out= None)\n",
        "    return predictions,enc_mean,rec_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB4LqOV5XVcx"
      },
      "source": [
        "predict_train, mu , re_mu = predict(model, train_loader)\n",
        "predict_anomaly, mu_anomaly, re_mu_anomaly= predict(model, anomaly_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzbQ2_QnX35B"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def projections(normal,anomaly):\n",
        "  mn = pd.DataFrame(normal)\n",
        "  mn['label']='Normal'\n",
        "  ma = pd.DataFrame(anomaly)\n",
        "  ma['label']='Anomaly'\n",
        "  m = mn.append(ma)\n",
        "\n",
        "\n",
        "  data_subset = m.iloc[:,0:10].values\n",
        "  time_start = time.time()\n",
        "  tsne = TSNE(n_components=2, verbose=1, perplexity=20, n_iter=1000)\n",
        "  tsne_results = tsne.fit_transform(data_subset)\n",
        "  print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "\n",
        "  m['Projection-1'] = tsne_results[:,0]\n",
        "  m['Projection-2'] = tsne_results[:,1]\n",
        "  plt.figure(figsize=(16,10))\n",
        "  sns.scatterplot(\n",
        "    x='Projection-1', y='Projection-2',\n",
        "    hue=\"label\",\n",
        "    palette=sns.color_palette(\"hls\", 2),\n",
        "    data=m,\n",
        "    legend=\"full\",\n",
        "    alpha=0.9)\n",
        "  #plt.savefig('/content/drive/MyDrive/AdvancedML/data/ecg_encoder_latent.png')\n",
        "  plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKYWoXdhaBca"
      },
      "source": [
        "#Projections encoder\n",
        "projections(mu,mu_anomaly)\n",
        "# Projections re-encoder\n",
        "projections(re_mu,re_mu_anomaly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg7cKg700WvP"
      },
      "source": [
        "def lillefor(data,predict,test=True):\n",
        "  if test:\n",
        "    #Lillefors test on test normal\n",
        "    residual = (data - predict)\n",
        "    ksstat = np.zeros(residual.shape[0])\n",
        "    for i in range(residual.shape[0]):\n",
        "      ksstat[i] = statsmodels.stats.diagnostic.lilliefors(residual[i,], dist='norm', pvalmethod='table')[0]\n",
        "    sns.distplot(ksstat, bins=40, kde=True)\n",
        "    #plt.savefig('/content/drive/MyDrive/AdvancedML/data/bigru_testnormal_rmse.png')\n",
        "    plt.show()\n",
        "    return ksstat\n",
        "  else:\n",
        "    #MSE on test normal\n",
        "    sq_loss = np.mean(( data- predict)**2, axis=1)\n",
        "    sns.distplot(test_sq_loss, bins=40, kde=True)\n",
        "    plt.show()\n",
        "    return sq_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzcOaqNgaNIa"
      },
      "source": [
        "train_df = train_df[:predict_train.shape[0]]\n",
        "train_original = np.squeeze(train_df,axis=2)\n",
        "train_recon = lillefor(train_original,predict_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZW6dFi50WvP"
      },
      "source": [
        "predict_test,_,_ = predict(model, test_loader)\n",
        "test_df=test_df[:predict_test.shape[0]]\n",
        "test_original = np.squeeze(test_df,axis=2)\n",
        "test_recon = lillefor(test_original,predict_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W51vegfOzFk"
      },
      "source": [
        "anomaly_df = anomaly_df[:predict_anomaly.shape[0]]\n",
        "anomaly_original = np.squeeze(anomaly_df)\n",
        "anomaly_recon = lillefor(anomaly_original,predict_anomaly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW2tlp330WvQ"
      },
      "source": [
        "def plot_prediction(data, reconstructed, title, ax):\n",
        "  ax.plot(data,'r', label='true')\n",
        "  ax.plot(reconstructed, 'b',label='reconstructed')\n",
        "  ax.set_title(f'{title} (Loss: {np.around(np.sum(( data - reconstructed)**2), 2)})')\n",
        "  ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXCQYLO-0WvR"
      },
      "source": [
        "fig, axs = plt.subplots(nrows=2,ncols=6,sharey=True,sharex=True,figsize=(22, 8))\n",
        "\n",
        "for i,data in enumerate(test_original[10:16]):\n",
        "    plot_prediction(data, predict_test[i], title='Normal', ax=axs[0, i])\n",
        "\n",
        "for i, data in enumerate(anomaly_original[10:16]):\n",
        "    plot_prediction(data, predict_anomaly[i], title='Anomaly', ax=axs[1, i])\n",
        "\n",
        "fig.tight_layout();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhvdWoksDt29"
      },
      "source": [
        "label = np.concatenate((np.zeros(len(test_recon)),np.ones(len(anomaly_recon))))\n",
        "#total_losses = np.concatenate((train_sq_loss,anomaly_sq_loss),axis=0)\n",
        "total_recon = np.concatenate((test_recon,anomaly_recon),axis=0)\n",
        "error_df = pd.DataFrame({'Reconstruction_error': total_recon,'Label':label})\n",
        "#error_df = error_df.sample(frac=1)\n",
        "error_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xGim6_lDw0O"
      },
      "source": [
        "false_pos_rate, true_pos_rate, thresholds = roc_curve(error_df.Label, error_df.Reconstruction_error.values)\n",
        "roc_auc = auc(false_pos_rate, true_pos_rate,)\n",
        "plt.plot(false_pos_rate, true_pos_rate, linewidth=2, label='AUC = %0.3f'% roc_auc)\n",
        "plt.plot([1,0],[1,0], linewidth=2)\n",
        "plt.xlim([-0.01, 1])\n",
        "plt.ylim([0, 1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Receiver operating characteristic curve (ROC)')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}